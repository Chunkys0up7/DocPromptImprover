# Evaluation-Only Framework for Prompt-Driven Document Extraction

## ğŸ¯ Overview

This is an **evaluation-only microservice** that assesses the outputs of existing OCR-plus-prompt pipelines using **pure statistical analysis**. The framework is completely decoupled from upstream OCR systems and focuses on metrics, statistics, and data-driven insights for manual prompt improvement.

**Key Features:**
- ğŸ” **Statistical Evaluation**: Rule-based scoring algorithms for different field types
- ğŸ“Š **Performance Analysis**: Comprehensive metrics and trend analysis
- ğŸ¯ **Pattern Detection**: Automated identification of failure patterns
- ğŸ“ˆ **Data-Driven Insights**: Actionable recommendations for prompt improvement
- ğŸ‘¥ **User Feedback Collection**: Collect and analyze user feedback on extraction results
- ğŸš¨ **Real-Time Alerts**: Automated alerts for performance issues
- ğŸ“ˆ **Trend Analysis**: Track performance changes over time
- ğŸš« **No LLM Dependencies**: Pure statistical analysis without AI/ML requirements

## ğŸš€ Quick Start Demo

**Want to see it in action?** Check out our comprehensive demo guide:

ğŸ“– **[DEMO_GUIDE.md](DEMO_GUIDE.md)** - Complete step-by-step instructions to run the feedback API demo with visualization

**Demo Features:**
- âœ… **Interactive Dashboard** - Web-based visualization with Streamlit
- âœ… **API Test Suite** - Programmatic demonstration of all endpoints
- âœ… **Real-time Statistics** - Live feedback analysis and alerts
- âœ… **Sample Data Generation** - Realistic test data for demonstration

## âœ… Implementation Status

**ğŸ‰ ALL PHASES COMPLETED!** The evaluation framework is now fully implemented and ready for production use.

- âœ… **Phase 1**: Core Evaluation Framework
- âœ… **Phase 2**: Evaluation Pipeline  
- âœ… **Phase 3**: Statistics & Monitoring
- âœ… **Phase 4**: Optimization Engine
- âœ… **Phase 5**: API & Integration

**Key Features Implemented:**
- ğŸ” Field-level evaluation with multiple scoring algorithms
- ğŸ“Š Document-level aggregation and performance analysis
- ğŸ¯ Error pattern detection and optimization recommendations
- ğŸ“ˆ Comprehensive statistics and trend analysis
- ğŸ‘¥ User feedback collection and analysis system
- ğŸš¨ Real-time alerting for performance issues
- ğŸ“ˆ Historical trend analysis and reporting
- ğŸ–¥ï¸ CLI interface and FastAPI service
- ğŸ§ª Complete test suite with unit and integration tests
- ğŸ“š Full documentation and usage examples

## ğŸ—ï¸ Architecture

### Key Design Principles

- **Evaluation-Only**: Never touches pixel data or performs OCR
- **Decoupled**: Works with any existing OCR-plus-prompt pipeline
- **Lightweight**: CPU-only instances, scales with JSON throughput
- **Metrics-Driven**: Provides quantifiable performance insights
- **Feedback Loops**: Enables continuous prompt optimization

### System Components

| Component | Purpose | Technology |
|-----------|---------|------------|
| **Ingestion Layer** | Parse evaluation inputs | Pydantic models |
| **Field Evaluator** | Score individual fields | Statistical algorithms |
| **Document Aggregator** | Combine field scores | Statistical metrics |
| **Statistics Store** | Persist evaluation data | Database + Pydantic |
| **Pattern Detector** | Identify failure patterns | Statistical analysis |
| **Recommendation Engine** | Generate improvement suggestions | Rule-based analysis |
| **Feedback Collector** | Collect user feedback | Pydantic validation |
| **Alert System** | Monitor performance issues | Threshold-based alerts |
| **Trend Analyzer** | Track performance over time | Time-series analysis |

## ğŸ“ Project Structure

```
doc-prompt-improvement/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ CORRECTED_IMPLEMENTATION_PLAN.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ evaluation_models.py         # Pydantic models for evaluation
â”‚   â”‚   â””â”€â”€ feedback_models.py           # Pydantic models for feedback
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ field_evaluator.py           # Field-level evaluation
â”‚   â”‚   â”œâ”€â”€ document_aggregator.py       # Document aggregation
â”‚   â”‚   â””â”€â”€ error_pattern_detector.py    # Error pattern detection
â”‚   â”œâ”€â”€ feedback/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ feedback_collector.py        # Feedback collection and analysis
â”‚   â”œâ”€â”€ statistics/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ statistics_engine.py         # Statistics and metrics
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ evaluation_service.py        # FastAPI evaluation service
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main.py                      # CLI interface
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                    # Configuration management
â”‚       â””â”€â”€ logging.py                   # Structured logging
â”œâ”€â”€ demos/
â”‚   â””â”€â”€ evaluation_demo.py               # Demo showcasing evaluation capabilities
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ end_to_end/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ samples/                         # Sample evaluation data
â”‚   â””â”€â”€ schemas/                         # Document schemas
â””â”€â”€ scripts/
    â””â”€â”€ generate_demo_data.py            # Data generation utilities
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone the repository
git clone <repository-url>
cd doc-prompt-improvement

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env

# Edit .env with your API keys
# OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
```

### 2. Run the Demo

```bash
# Run the evaluation demo
python demos/evaluation_demo.py

# Or use the CLI
python -m src.cli.main demo
```

### 3. Start the Evaluation Service

```bash
# Start the FastAPI service
python -m src.api.evaluation_service

# Or use the CLI
python -m src.cli.main serve --port 8000
```

The service will be available at `http://localhost:8000`

### 4. CLI Commands

```bash
# Run evaluation with dummy data
python -m src.cli.main evaluate

# Generate dummy data
python -m src.cli.main generate-data --num-documents 100

# Analyze results
python -m src.cli.main analyze --input-file results.json --output-file analysis.json

# Run tests
python -m src.cli.main test --coverage

# Run comprehensive demo
python -m src.cli.main demo
```

### 5. API Endpoints

- `POST /evaluate` - Evaluate document extraction results
- `GET /stats` - Get evaluation statistics
- `POST /optimize` - Generate prompt optimization recommendations
- `GET /health` - Health check
- `GET /config` - Get configuration
- `POST /config` - Update configuration
- `POST /reset` - Reset statistics

## ğŸ“Š Usage Examples

### Evaluate Document Extraction

```python
from src.models.evaluation_models import DocumentEvaluationInput
from src.api.evaluation_service import DocumentExtractionEvaluator

# Create evaluation input
evaluation_input = DocumentEvaluationInput(
    document_id="invoice_001",
    document_type="invoice",
    extracted_fields={
        "vendor_name": "Acme Corporation",
        "invoice_number": "INV-2024-001",
        "total_amount": "1250.00"
    },
    ground_truth={
        "vendor_name": "Acme Corporation",
        "invoice_number": "INV-2024-001", 
        "total_amount": "1250.00"
    },
    confidence_scores={
        "vendor_name": 0.95,
        "invoice_number": 0.98,
        "total_amount": 0.96
    }
)

# Initialize evaluator
evaluator = DocumentExtractionEvaluator()

# Perform evaluation
result = evaluator.evaluate_document(evaluation_input)
print(f"Overall accuracy: {result.overall_accuracy:.1%}")
```

### Generate Optimization Recommendations

```python
# Get optimization metrics
metrics = evaluator.get_optimization_metrics()

# Generate optimization recommendations
optimization_result = evaluator.optimizer(
    evaluation_statistics=json.dumps(metrics),
    failure_patterns=json.dumps(common_errors),
    current_prompt=current_prompt,
    target_improvement=0.1
)

print(f"Optimized prompt: {optimization_result.optimized_prompt}")
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# Service Configuration
EVALUATION_SERVICE_PORT=8000
EVALUATION_SERVICE_HOST=0.0.0.0

# LLM Configuration
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
DEFAULT_LLM_PROVIDER=openai

# Evaluation Parameters
CONFIDENCE_THRESHOLD=0.7
SUCCESS_THRESHOLD=0.8
OPTIMIZATION_INTERVAL=100

# Monitoring
METRICS_ENABLED=true
ALERT_THRESHOLD=0.75
```

### Evaluation Configuration

```python
from src.models.evaluation_models import EvaluationConfig

config = EvaluationConfig(
    confidence_threshold=0.7,
    success_threshold=0.8,
    partial_threshold=0.5,
    enable_partial_credit=True,
    strict_matching=False,
    case_sensitive=False,
    normalize_whitespace=True
)
```

## ğŸ“ˆ Performance Metrics

### Evaluation Accuracy
- **Field-level precision**: Target 95%+
- **Recall accuracy**: Target 90%+
- **Confidence calibration**: Target 85%+ correlation
- **Error detection**: Target 90%+ failure identification

### Processing Performance
- **Evaluation speed**: < 100ms per document
- **Throughput**: > 1000 documents per minute
- **Memory efficiency**: < 1GB RAM for batch processing
- **CPU utilization**: < 80% under normal load

### Optimization Effectiveness
- **Improvement rate**: > 10% accuracy improvement per cycle
- **Convergence time**: < 5 optimization cycles
- **Prompt quality**: Measurable improvement in extraction accuracy
- **Feedback loop**: < 24 hours from evaluation to optimization

## ğŸ§ª Testing

### Run Tests

```bash
# Run unit tests
python -m pytest tests/unit/

# Run integration tests
python -m pytest tests/integration/

# Run end-to-end tests
python -m pytest tests/end_to_end/
```

### Performance Testing

```bash
# Run performance benchmarks
python scripts/run_benchmarks.py
```

## ğŸš€ Deployment

### Development

```bash
# Local development with hot reload
uvicorn src.api.evaluation_service:app --reload --host 0.0.0.0 --port 8000
```

### Production

```bash
# Using Docker
docker build -t evaluation-service .
docker run -p 8000:8000 evaluation-service

# Using Kubernetes
kubectl apply -f k8s/
```

## ğŸ“‹ Implementation Status

### âœ… Phase 1: Core Evaluation Framework (COMPLETED)
- [x] Pydantic data models for evaluation
- [x] DSPy evaluation signatures
- [x] Basic evaluation pipeline
- [x] Configuration management
- [x] Structured logging
- [x] FastAPI service
- [x] Demo application

### âœ… Phase 2: Evaluation Pipeline (COMPLETED)
- [x] Field-level evaluation logic
- [x] Document-level aggregation
- [x] Confidence scoring algorithms
- [x] Error pattern detection
- [x] Evaluation result persistence

### âœ… Phase 3: Statistics & Monitoring (COMPLETED)
- [x] Statistics collection engine
- [x] Performance dashboards
- [x] Trend analysis
- [x] Alert systems
- [x] Data persistence layer

### âœ… Phase 4: Optimization Engine (COMPLETED)
- [x] DSPy optimizer integration
- [x] Failure pattern analysis
- [x] Prompt improvement generation
- [x] Optimization feedback loops
- [x] A/B testing support

### âœ… Phase 5: API & Integration (COMPLETED)
- [x] FastAPI microservice
- [x] REST API endpoints
- [x] Integration documentation
- [x] Deployment configuration
- [x] Performance optimization

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:
- Create an issue in the repository
- Check the documentation
- Review the demo examples

---

*This framework provides a lightweight, decoupled evaluation service that transforms vague prompt tweaking into a rigorous, metric-driven discipline.* 